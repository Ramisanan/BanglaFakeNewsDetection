{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n    Package Installation\n\"\"\"\n__author__ = \"SakibApon\"\n__email__ = \"sakibapon7@gmail.com\"\n\nimport os\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\ntf.gfile = tf.io.gfile\n\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau\n\nSEED = 1337","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-22T17:51:40.595358Z","iopub.execute_input":"2022-08-22T17:51:40.596027Z","iopub.status.idle":"2022-08-22T17:51:40.607444Z","shell.execute_reply.started":"2022-08-22T17:51:40.595990Z","shell.execute_reply":"2022-08-22T17:51:40.605714Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessed Dataset Input","metadata":{}},{"cell_type":"code","source":"#loading datasets\ndataset_01 = pd.read_csv(\"/kaggle/input/banfakenews/LabeledAuthentic-7K.csv\")\ndataset_02 = pd.read_csv(\"/kaggle/input/banfakenews/LabeledFake-1K.csv\")\n\n# combining two datasets\ndf_train_ =  pd.concat([dataset_01, dataset_02])\nprint(len(df_train_))\ndf_train_.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:58:21.688376Z","iopub.execute_input":"2022-08-22T17:58:21.689368Z","iopub.status.idle":"2022-08-22T17:58:22.243165Z","shell.execute_reply.started":"2022-08-22T17:58:21.689315Z","shell.execute_reply":"2022-08-22T17:58:22.242185Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_train = df_train_[['headline', 'label']]\nprint(len(df_train))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:58:23.691840Z","iopub.execute_input":"2022-08-22T17:58:23.692473Z","iopub.status.idle":"2022-08-22T17:58:23.718948Z","shell.execute_reply.started":"2022-08-22T17:58:23.692436Z","shell.execute_reply":"2022-08-22T17:58:23.717910Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_train.rename(columns={'headline': 'Comments',  'label': 'Label'},  inplace=True, errors='raise')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:58:25.061554Z","iopub.execute_input":"2022-08-22T17:58:25.062070Z","iopub.status.idle":"2022-08-22T17:58:25.096572Z","shell.execute_reply.started":"2022-08-22T17:58:25.062025Z","shell.execute_reply":"2022-08-22T17:58:25.095169Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv(\"processed.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:58:41.468788Z","iopub.execute_input":"2022-08-22T17:58:41.469787Z","iopub.status.idle":"2022-08-22T17:58:41.499610Z","shell.execute_reply.started":"2022-08-22T17:58:41.469746Z","shell.execute_reply":"2022-08-22T17:58:41.498706Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"./processed.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:59:23.913911Z","iopub.execute_input":"2022-08-22T17:59:23.914403Z","iopub.status.idle":"2022-08-22T17:59:23.960837Z","shell.execute_reply.started":"2022-08-22T17:59:23.914369Z","shell.execute_reply":"2022-08-22T17:59:23.959830Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### K-fold","metadata":{}},{"cell_type":"code","source":"K = 4 # K-fold\nskf = StratifiedKFold(n_splits= K, random_state=SEED, shuffle=True)\n\nDISASTER = df_train['Label'] == 1\nprint('Whole Training Set Shape = {}'.format(df_train.shape))\nprint('Whole Training Set Unique keyword Count = {}'.format(df_train['Label'].nunique()))\nprint('Whole Training Set Target Rate (Disaster) {}/{} (Not Disaster)'.format(df_train[DISASTER]['Label'].count(), df_train[~DISASTER]['Label'].count()))\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['Comments'], df_train['Label']), 1):\n    print('\\nFold {} Training Set Shape = {} - Validation Set Shape = {}'.format(fold, df_train.loc[trn_idx, 'Comments'].shape, df_train.loc[val_idx, 'Comments'].shape))\n    print('Fold {} Training Set Unique keyword Count = {} - Validation Set Unique keyword Count = {}'.format(fold, df_train.loc[trn_idx, 'Label'].nunique(), df_train.loc[val_idx, 'Label'].nunique()))    ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:59:31.555487Z","iopub.execute_input":"2022-08-22T17:59:31.555835Z","iopub.status.idle":"2022-08-22T17:59:31.584991Z","shell.execute_reply.started":"2022-08-22T17:59:31.555805Z","shell.execute_reply":"2022-08-22T17:59:31.583903Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# BERT ","metadata":{}},{"cell_type":"markdown","source":"### Module Installation ","metadata":{}},{"cell_type":"code","source":"!pip install bert-tensorflow==1.0.1","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:56:36.733649Z","iopub.execute_input":"2022-08-22T17:56:36.734255Z","iopub.status.idle":"2022-08-22T17:56:48.299001Z","shell.execute_reply.started":"2022-08-22T17:56:36.734215Z","shell.execute_reply":"2022-08-22T17:56:48.297835Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### BERT Layer","metadata":{}},{"cell_type":"code","source":"%%time\nbert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:57:04.166577Z","iopub.execute_input":"2022-08-22T17:57:04.167477Z","iopub.status.idle":"2022-08-22T17:57:18.824224Z","shell.execute_reply.started":"2022-08-22T17:57:04.167439Z","shell.execute_reply":"2022-08-22T17:57:18.822380Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### BERT Architecture","metadata":{}},{"cell_type":"code","source":"class DisasterDetector:\n    \n    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n        \n        \n    def encode(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    \n    \n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs=out)\n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n        #tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()\n        model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = metrics)\n        \n        return model\n    \n    \n    def train(self, X):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['Comments'], X['Label'])):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.encode(X.loc[trn_idx, 'Comments'].str.lower())\n            y_trn = X.loc[trn_idx, 'Label']\n            X_val_encoded = self.encode(X.loc[val_idx, 'Comments'].str.lower())\n            y_val = X.loc[val_idx, 'Label']\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            checkpoint = ModelCheckpoint('SarcasmBERT.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n            earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, restore_best_weights=True)\n            learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.2, min_lr=0.001)\n            \n            callbacks = [earlystop, checkpoint, learning_rate_reduction, metrics]\n            \n            # Model\n            model = self.build_model()        \n            history = model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks = callbacks, epochs=self.epochs, batch_size=self.batch_size)\n            \n            #plot_graphs(history, \"accuracy\")\n            #plot_graphs(history, \"loss\")\n            \n            \"\"\"y_pred = argmax(model.predict(X_val_encoded),axis=1)\n            y_true = y_val\n            target_names = ['class 0', 'class 1']\n            print(classification_report(y_true, y_pred, target_names=target_names))\n\n            cm = confusion_matrix(y_true, y_pred)\n            f = sns.heatmap(cm, annot=True, fmt='d')\n            \n            #cm(y_pred)\n            \n            probs = model.predict_proba(X_val_encoded)\n            preds = probs[:,0]\n            fpr, tpr, threshold = metrics.roc_curve(testing_labels, preds)\n            roc_auc = metrics.auc(fpr, tpr)\n\n            import matplotlib.pyplot as plt\n            plt.title('Receiver Operating Characteristic')\n            plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n            plt.legend(loc = 'lower right')\n            plt.plot([0, 1], [0, 1],'r--')\n            plt.xlim([0, 1])\n            plt.ylim([0, 1])\n            plt.ylabel('True Positive Rate')\n            plt.xlabel('False Positive Rate')\n            plt.show()\n            \n            #roc(probs)\"\"\"\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n                    \n                \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train / Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n        \n    def predict(self, X):\n        \n        X_test_encoded = self.encode(X['Comments'].str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) / len(self.models)\n\n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:59:58.717271Z","iopub.execute_input":"2022-08-22T17:59:58.717682Z","iopub.status.idle":"2022-08-22T17:59:58.749179Z","shell.execute_reply.started":"2022-08-22T17:59:58.717650Z","shell.execute_reply":"2022-08-22T17:59:58.748215Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Classification Report on epoch end","metadata":{}},{"cell_type":"code","source":"class ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T17:59:59.952778Z","iopub.execute_input":"2022-08-22T17:59:59.953469Z","iopub.status.idle":"2022-08-22T17:59:59.964277Z","shell.execute_reply.started":"2022-08-22T17:59:59.953428Z","shell.execute_reply":"2022-08-22T17:59:59.963233Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### Initialize Training","metadata":{}},{"cell_type":"code","source":"from bert import tokenization\n\nclf = DisasterDetector(bert_layer, max_seq_length=128, lr = 0.0001, epochs = 10, batch_size = 32)\nclf.train(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T18:00:01.312356Z","iopub.execute_input":"2022-08-22T18:00:01.312971Z","iopub.status.idle":"2022-08-22T19:43:24.517847Z","shell.execute_reply.started":"2022-08-22T18:00:01.312937Z","shell.execute_reply":"2022-08-22T19:43:24.516766Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation Graphs","metadata":{}},{"cell_type":"code","source":"clf.plot_learning_curve()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:43:24.520086Z","iopub.execute_input":"2022-08-22T19:43:24.520498Z","iopub.status.idle":"2022-08-22T19:43:26.223687Z","shell.execute_reply.started":"2022-08-22T19:43:24.520463Z","shell.execute_reply":"2022-08-22T19:43:26.222435Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}