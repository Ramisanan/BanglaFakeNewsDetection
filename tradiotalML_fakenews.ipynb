{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re, nltk, os, json\nfrom flask import jsonify\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-10T16:56:39.012210Z","iopub.execute_input":"2022-08-10T16:56:39.012990Z","iopub.status.idle":"2022-08-10T16:56:39.029899Z","shell.execute_reply.started":"2022-08-10T16:56:39.012931Z","shell.execute_reply":"2022-08-10T16:56:39.028984Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#loading datasets\ndataset_01 = pd.read_csv(\"/kaggle/input/banfakenews/LabeledAuthentic-7K.csv\")\ndataset_02 = pd.read_csv(\"/kaggle/input/banfakenews/LabeledFake-1K.csv\")\n\n# combining two datasets\ndf =  pd.concat([dataset_01, dataset_02])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:39.119725Z","iopub.execute_input":"2022-08-10T16:56:39.120422Z","iopub.status.idle":"2022-08-10T16:56:39.792138Z","shell.execute_reply.started":"2022-08-10T16:56:39.120372Z","shell.execute_reply":"2022-08-10T16:56:39.790856Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df.drop(['articleID', 'date', 'domain', 'source', 'relation', 'F-type'], axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:39.793957Z","iopub.execute_input":"2022-08-10T16:56:39.794290Z","iopub.status.idle":"2022-08-10T16:56:39.812724Z","shell.execute_reply.started":"2022-08-10T16:56:39.794260Z","shell.execute_reply":"2022-08-10T16:56:39.811425Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlb_make = LabelEncoder()\ndf['category'] = lb_make.fit_transform(df['category'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:39.814111Z","iopub.execute_input":"2022-08-10T16:56:39.814682Z","iopub.status.idle":"2022-08-10T16:56:39.832762Z","shell.execute_reply.started":"2022-08-10T16:56:39.814639Z","shell.execute_reply":"2022-08-10T16:56:39.831869Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"vocab_size = 10000 # max_features \ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:39.835093Z","iopub.execute_input":"2022-08-10T16:56:39.835634Z","iopub.status.idle":"2022-08-10T16:56:39.860903Z","shell.execute_reply.started":"2022-08-10T16:56:39.835577Z","shell.execute_reply":"2022-08-10T16:56:39.859749Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(df.content)\ntrain_inputs = tokenizer.texts_to_sequences(df.content)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:39.863281Z","iopub.execute_input":"2022-08-10T16:56:39.864371Z","iopub.status.idle":"2022-08-10T16:56:46.973296Z","shell.execute_reply.started":"2022-08-10T16:56:39.864280Z","shell.execute_reply":"2022-08-10T16:56:46.972102Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(df.headline)\ntrain_inputs_headline = tokenizer.texts_to_sequences(df.headline)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:46.975140Z","iopub.execute_input":"2022-08-10T16:56:46.975586Z","iopub.status.idle":"2022-08-10T16:56:47.608418Z","shell.execute_reply.started":"2022-08-10T16:56:46.975542Z","shell.execute_reply":"2022-08-10T16:56:47.607464Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_inputs = tf.keras.preprocessing.sequence.pad_sequences(train_inputs, padding='post', maxlen=20)\ntrain_labels = df['label']\nscore = []\n# Split data into train /validation \nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_inputs, train_labels, test_size=0.2, random_state=0)\ntrain_inputs[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.609932Z","iopub.execute_input":"2022-08-10T16:56:47.610246Z","iopub.status.idle":"2022-08-10T16:56:47.667493Z","shell.execute_reply.started":"2022-08-10T16:56:47.610209Z","shell.execute_reply":"2022-08-10T16:56:47.666460Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def cleaned_reviews(review):\n    \"\"\"\n    This function will remove the unncessary \n    symbols from a review such as punctuation mark, numbers ,emoji. \n    \n    Args:\n        review: str\n        \n    Returns:\n        cleaned review: str\n    \"\"\"\n    review = review.replace('\\n', '') #removing new line \n    review = re.sub('[^\\u0980-\\u09FF]',' ',str(review)) #removing unnecessary punctuation\n    return review\n                                                 #=========================== \n                                                 ####### Function 2 #########\n                                                 #===========================  \ndef stopwords_info(filename):\n    \"\"\"\n    This function will create a stopwords list from the (.txt) file. \n    \n    Args:\n        filename: str\n        \n    Returns:\n        stp: list of stopwords\n        num_of_words: int\n    \"\"\"\n    stp = open(filename,'r',encoding='utf-8').read().split()\n    num_of_stopwords = len(stp)\n    return stp,num_of_stopwords\n\n                                                 #=========================== \n                                                 ####### Function 3 #########\n                                                 #===========================  \ndef stopword_removal(review,stopwords):\n    \"\"\"\n    This function will remove the stopwords from a review. \n    \n    Args:\n        review: str\n        stopwords: str\n        \n    Returns:\n        review without stopwords: str\n    \"\"\"\n    stp,num_of_stopwords =stopwords_info(stopwords)\n    result = review.split()\n    reviews = [word.strip() for word in result if word not in stp ]\n    reviews =\" \".join(reviews)\n    return reviews\n                                                 #=========================== \n                                                 ####### Function 4 #########\n                                                 #===========================               \ndef process_reviews(review,stopwords,removing_stopwords):\n    \"\"\"\n    This function will process all the reviews which includes\n    cleaning text and stopword removal.\n    \n    Args:\n        review: str\n        stopwords: filename\n        removing_stopwords: boolean(True or False) \n        \n    Returns:\n        cleaned reviews: str\n    \"\"\"\n    if removing_stopwords ==False:\n        reviews = cleaned_reviews(review)\n        \n    else:\n        reviews = cleaned_reviews(review)\n        reviews = stopword_removal(reviews,stopwords)\n        \n    return reviews    \n\n\n\n                                                 #=========================== \n                                                 ####### Function 5 #########\n                                                 #===========================  \n\ndef data_summary(dataset):\n    \n    \"\"\"\n    This function will print the summary of the reviews and words distribution in the dataset. \n    \n    Args:\n        dataset: list of cleaned sentences   \n        \n    Returns:\n        Number of documnets per class: int \n        Number of words per class: int\n        Number of unique words per class: int\n    \"\"\"\n    documents = []\n    words = []\n    u_words = []\n    class_label= [k for k,v in dataset.headline.value_counts().to_dict().items()]\n  # find word list\n    for label in class_label: \n        word_list = [word.strip().lower() for t in list(dataset[dataset.label==label].headlline) for word in t.strip().split()]\n        counts = dict()\n        for word in word_list:\n                counts[word] = counts.get(word, 0)+1\n    # sort the dictionary of word list  \n        ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)\n    # Documents per class\n        documents.append(len(list(dataset[dataset.label==label].headline)))\n    # Total Word per class\n        words.append(len(word_list))\n    # Unique words per class \n        u_words.append(len(np.unique(word_list)))\n       \n        print(\"\\nClass Name : \",label)\n        print(\"Number of Documents:{}\".format(len(list(dataset[dataset.label==label].headline))))  \n        print(\"Number of Words:{}\".format(len(word_list))) \n        print(\"Number of Unique Words:{}\".format(len(np.unique(word_list)))) \n        print(\"Most Frequent Words:\\n\")\n        for k,v in ordered[:10]:\n              print(\"{}\\t{}\".format(k,v))\n   \n    return documents,words,u_words,class_label\n\n\n                                       #==================================================\n                                       ################# Label Encoding Function #########\n                                       #==================================================\ndef label_encoding(sentiment,bool):\n    \"\"\"\n    This function will return the encoded labels in array format. \n    \n    Args:\n        sentiment: series of class names(str)\n        bool: boolean (True or False)\n        \n    Returns:\n        labels: numpy array \n    \"\"\"\n    le = LabelEncoder()\n    le.fit(sentiment)\n    encoded_labels = le.transform(sentiment)\n    labels = np.array(encoded_labels) # Converting into numpy array\n    class_names =le.classes_ ## Define the class names again\n    if bool == True:\n        print(\"\\n\\t\\t\\t===== Label Encoding =====\",\"\\nClass Names:-->\",le.classes_)\n        for i in sample_data:\n            print(sentiment[i],' ', encoded_labels[i],'\\n')\n\n    return labels  \n\n\n                           #===========================================================\n                           ################# Dataset Splitting Function ###############\n                           #=========================================================== \n\ndef dataset_split(feature_space,sentiment):\n    \"\"\"\n    This function will return the splitted (80%-20%) feature vector . \n    \n    Args:\n        feature_space: calcuated feature vector (sparse matrix)\n        sentiment: encoded lables (array) \n        \n    Returns:\n        X_train: training feature vector (sparse matrix)\n        X_test : testing feature vector (sparse matrix)\n        y_train: training encoded labels (array) \n        y_test : testing encoded labels (array) \n    \"\"\"\n\n    X_train,X_test,y_train,y_test = train_test_split(feature_space,sentiment,train_size = 0.8,\n                                                  test_size = 0.2,random_state =0)\n    print(\"Feature Size :======>\",X_train.shape[1])\n    print(\"\\nDataset Distribution:\\n\")\n    print(\"\\tSet Name\",\"\\t\\tSize\")\n    print(\"\\t========\\t\\t======\")\n\n    print(\"\\tFull\\t\\t\\t\",feature_space.shape[0],\n        \"\\n\\tTraining\\t\\t\",X_train.shape[0],\n        \"\\n\\tTest\\t\\t\\t\",X_test.shape[0])\n  \n    return X_train,X_test,y_train,y_test\n\n                                            #======================================\n                                            ##### Unigram Tf-idf value calculation\n                                            #======================================\n            \ndef calc_unigram_tfidf(reviews):\n    \"\"\"\n    This function will return the tf-idf value of the unigram features . \n    \n    Args:\n        reviews: a list of cleaned reviews   \n        \n    Returns:\n        tfidf: a instance of TfidfVectorizer\n        X : Unigram Feature Vector (sparse matrix)\n    \"\"\"\n    tfidf = TfidfVectorizer(use_idf=True,tokenizer=lambda x: x.split()) \n    X = tfidf.fit_transform(reviews)\n    \n    return tfidf,X\n\n                                            #======================================\n                                            ##### Bi-gram Tf-idf value calculation\n                                            #======================================\n            \ndef calc_bigram_tfidf(reviews):\n    \"\"\"\n    This function will return the tf-idf value of the bigram features . \n    \n    Args:\n        reviews: a list of cleaned reviews   \n        \n    Returns:\n        tfidf: a instance of TfidfVectorizer\n        X : Bigram Feature Vector (sparse matrix)\n    \"\"\"\n    tfidf = TfidfVectorizer(ngram_range=(1,2),use_idf=True,tokenizer=lambda x: x.split()) \n    X = tfidf.fit_transform(reviews)\n    \n    return tfidf,X\n\n                                            #======================================\n                                            ##### Tri-gram Tf-idf value calculation\n                                            #======================================\n\n\ndef calc_trigram_tfidf(reviews):\n    \"\"\"\n    This function will return the tf-idf value of the bigram features . \n    \n    Args:\n        reviews: a list of cleaned reviews   \n        \n    Returns:\n        tfidf: a instance of TfidfVectorizer\n        X : Tri-gram Feature Vector (sparse matrix)\n    \"\"\"\n    tfidf = TfidfVectorizer(ngram_range=(1,3),use_idf=True,tokenizer=lambda x: x.split()) \n    X = tfidf.fit_transform(reviews)\n    \n    return tfidf,X\n\n                                    #=======================\n                                    ##### Print Tf-idf value\n                                    #=======================\n\ndef show_tfidf(tfidf_instance,samp_review): \n    \"\"\"\n    This function will print the tfidf value of a specific review . \n    \n    Args:\n        tfidf_instance: an object\n        samp_review   : string\n        \n    Returns:\n        tfidf value of gram feature \n    \"\"\"\n    print(\"Sample Review: \",samp_review)\n    first_vector = tfidf_instance.transform([samp_review])\n    df = pd.DataFrame(first_vector.T.todense(), index=tfidf_instance.get_feature_names(), columns=[\"tfidf\"])\n    a = df.sort_values(by=[\"tfidf\"],ascending=False)\n    print(a[0:len(samp_review.split())])\n    \n    \n                                              #============================================\n                                              #########  Classifiers for Unigram  #########\n                                              #============================================\ndef ml_models_for_unigram_tfidf(): \n    \"\"\"\n    This function consists the models defination for Unigram Features\n    \n    Retuns:\n        ml_models: list of models\n        model_names: list of model_names\n    \n    \"\"\"\n    \n    lr_model = LogisticRegression(random_state = 123)\n    dt_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    rf_model = RandomForestClassifier(n_estimators=100, criterion ='entropy', random_state = 0)\n    mnb_model = MultinomialNB(alpha=0.15)\n    knn_model = KNeighborsClassifier(n_neighbors=3, metric = 'minkowski')\n    #lsvm_model = SVC(kernel = 'linear',C = 0.2, probability=True, random_state = 0)\n    ksvm_model = SVC(C= 1000,kernel = 'rbf',probability=True, gamma = 0.00015, random_state = 0)\n    sgd_model = SGDClassifier(loss ='log',penalty='l2', max_iter=5)\n    #model_names = ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','KNN','Linear SVM','Kernel SVM','SGD']\n    model_names = ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','KNN','Kernel SVM','SGD']\n    # Create list of models\n    #ml_models = [lr_model,dt_model,rf_model,mnb_model,knn_model,lsvm_model,ksvm_model,sgd_model]\n    ml_models = [lr_model,dt_model,rf_model,mnb_model,knn_model,ksvm_model,sgd_model]\n    return ml_models,model_names    \n\n    \n                                              #============================================\n                                              #########  Classifiers for Bigram  #########\n                                              #============================================\ndef ml_models_for_bigram_tfidf(): \n    \"\"\"\n    This function consists the models defination for Bigram Features\n    \n    Retuns:\n        ml_models: list of models\n        model_names: list of model_names\n    \n    \"\"\"\n    lr_model = LogisticRegression(random_state = 123)\n    dt_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    rf_model = RandomForestClassifier(n_estimators=100, criterion ='entropy', random_state = 0)\n    mnb_model = MultinomialNB(alpha=0.008)\n    knn_model = KNeighborsClassifier(n_neighbors=2, metric = 'minkowski')\n    lsvm_model = SVC(kernel = 'linear',C = 0.3, probability=True, random_state = 0)\n    ksvm_model = SVC(C= 1000,kernel = 'rbf',probability=True, gamma = 0.00015, random_state = 0)\n    sgd_model = SGDClassifier(loss = 'log',penalty='l2', max_iter=5)\n    model_names = ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','KNN','Linear SVM','Kernel SVM','SGD']\n    ml_models = [lr_model,dt_model,rf_model,mnb_model,knn_model,lsvm_model,ksvm_model,sgd_model]\n    return ml_models,model_names    \n\n\n                                              #============================================\n                                              #########  Classifiers for Trigram  #########\n                                              #============================================\ndef ml_models_for_trigram_tfidf(): \n    \"\"\"\n    This function consists the models defination for Tri-gram Features\n    \n    Retuns:\n        ml_models: list of models\n        model_names: list of model_names\n    \n    \"\"\"\n    \n    lr_model = LogisticRegression(random_state = 123)\n    dt_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    rf_model = RandomForestClassifier(n_estimators=100, criterion ='entropy', random_state = 0)\n    mnb_model = MultinomialNB(alpha=0.001)\n    knn_model = KNeighborsClassifier(n_neighbors=2, metric = 'minkowski')\n    lsvm_model = SVC(kernel = 'linear',C = 0.3, probability=True, random_state = 0)\n    ksvm_model = SVC(C= 1000,kernel = 'rbf',probability=True, gamma = 0.0002, random_state = 0)\n    sgd_model = SGDClassifier(loss = 'log',penalty='l2', max_iter=5)\n    model_names = ['Logistic Regression','Decision Tree','Random Forest','Naive Bayes','KNN','Linear SVM','Kernel SVM','SGD']\n    #create a list of models\n    ml_models = [lr_model,dt_model,rf_model,mnb_model,knn_model,ksvm_model,sgd_model]\n    return ml_models,model_names     \n    \n    \n                                           #============================================\n                                           ####### Model Evaluation Function ############\n                                           #=============================================\n\ndef model_performace(model,X_train,X_test,y_train,y_test):\n    \"\"\"\n    This function will return the performance parameter values of each ML models.\n    Performance parameters are Accuracy, F1-Score, Precision, Recall.\n    \n    Args:\n        model: a ML model instance\n        X_train: training feature vector (sparse matrix)\n        X_test : testing feature vector (sparse matrix)\n        y_train: training encoded labels (array) \n        y_test : testing encoded labels (array) \n        \n    Returns:\n        my_dict: a dictionary of all the parameters for each models\n    \"\"\"\n    my_dict = {}\n    model.fit(X_train,y_train)\n    # Prediction\n    pred_y = model.predict(X_test)\n    my_dict['Accuracy'] = round(accuracy_score(y_test, pred_y),4)*100 \n    my_dict['Precision'] = round(precision_score(y_test, pred_y),4)*100 \n    my_dict['Recall'] = round(recall_score(y_test, pred_y),4)*100 \n    my_dict['F1 Score'] = round(f1_score(y_test, pred_y),4)*100 \n    print(\"Model => {} Accuracy: {} Precision: {} Recall {} F1-Score {}\".format(model, my_dict['Accuracy'], my_dict['Precision'], my_dict['Recall'], my_dict['F1 Score'] ))\n    return my_dict  \n\n                                        #========================================\n                                        #### Model Performane into Dataframe #####\n                                        #=========================================\n            \ndef performance_table(performance_dict):\n    \"\"\"\n    This function will create a dataframe of all the performance parameters.\n    \n    Args:\n        performance_dict: a dictionary of all the parameters for each models\n        \n    Returns:\n        performance_df: a dataframe\n    \"\"\"\n\n    acc_list = []\n    pr_list = []\n    re_list = []\n    f1_list = []\n    for i in performance_dict.keys():\n        acc_list.append(performance_dict[i]['Accuracy'])\n        pr_list.append(performance_dict[i]['Precision'])\n        re_list.append(performance_dict[i]['Recall'])\n        f1_list.append(performance_dict[i]['F1 Score'])\n\n    # Create a dataframe\n    performance_df = pd.DataFrame({'Accuracy':acc_list,'Precision':pr_list,\n                                   'Recall':re_list,'F1 Score':f1_list},\n                                  index =['LR','DT','RF','MNB','KNN','RBF SVM','SGD'])\n    return performance_df\n\n                                        #============================\n                                        #### Plotting ROC Curve #####\n                                        #============================\n\n\n\ndef plot_roc_curve(gram_models,X_train,X_test,y_train,y_test,gram_name):\n    \n    \"\"\"\n    This function will plot the ROC curve for all classifiers\n    \n    Args:\n        gram_models: a function of all the models defination for a gram feature\n        X_train: training feature vector (sparse matrix)\n        X_test : testing feature vector (sparse matrix)\n        y_train: training encoded labels (array) \n        y_test : testing encoded labels (array) \n        gram_name: gram feature name(str)\n        \n    \n    \"\"\"\n    \n    ml_models,model_names = gram_models \n\n    # Define a result table as a DataFrame\n    result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n    # Train the models and record the results\n    for i,model in enumerate(ml_models):\n        model = model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)[::,1]\n\n        fpr, tpr, _ = roc_curve(y_test, y_pred)\n\n        auc = roc_auc_score(y_test, y_pred)\n\n        result_table = result_table.append({'classifiers':model_names[i],\n                                            'fpr':fpr, \n                                            'tpr':tpr, \n                                            'auc':auc}, ignore_index=True)\n\n    # Set name of the classifiers as index labels\n    result_table.set_index('classifiers', inplace=True)\n\n    # plotting\n    fig = plt.figure(figsize=(8,6))\n\n    for i in result_table.index:\n        plt.plot(result_table.loc[i]['fpr'], \n                 result_table.loc[i]['tpr'], \n                 label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n\n    plt.plot([0,1], [0,1], color='orange', linestyle='--')\n\n    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n    plt.xlabel(\"False Positive Rate\", fontsize=12)\n\n    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n    plt.ylabel(\"True Positive Rate\", fontsize=12)\n\n    plt.title(f'ROC Curve Analysis for {gram_name} features', fontweight='bold', fontsize=12)\n    plt.legend(prop={'size':13}, loc='lower right')\n\n    plt.show()\n    \n    \n                                        #=========================================\n                                        #### Plotting Precision-Recall Curve #####\n                                        #========================================    \n    \n    \n    \ndef plot_PR_curve(gram_models,X_train,X_test,y_train,y_test,gram_name):\n    \"\"\"\n    This function will plot the Precision Recall curve for all classifiers\n    \n    Args:\n        gram_models: a function of all the models defination for a gram feature\n        X_train: training feature vector (sparse matrix)\n        X_test : testing feature vector (sparse matrix)\n        y_train: training encoded labels (array) \n        y_test : testing encoded labels (array) \n        gram_name: gram feature name(str)\n        \n    \n    \"\"\"\n    \n    ml_models,model_names = gram_models \n    # Define a result table as a DataFrame\n    result_table = pd.DataFrame(columns=['classifiers', 'precision','recall','AP'])\n\n    # Train the models and record the results\n    for i,model in enumerate(ml_models):\n        model = model.fit(X_train, y_train)\n        y_pred = model.predict_proba(X_test)[::,1]\n\n        precision, recall,_ = precision_recall_curve(y_test, y_pred)\n\n        average_precision = average_precision_score(y_test, y_pred)\n\n        result_table = result_table.append({'classifiers':model_names[i],\n                                            'precision':precision, \n                                            'recall':recall, \n                                            'AP': average_precision}, ignore_index=True)\n\n    # Set name of the classifiers as index labels\n    result_table.set_index('classifiers', inplace=True)\n\n    # Plotting\n\n    fig = plt.figure(figsize=(8,6))\n\n    for i in result_table.index:\n        plt.plot(result_table.loc[i]['recall'], \n                 result_table.loc[i]['precision'], \n                 label=\"{}, AP={:.3f}\".format(i, result_table.loc[i]['AP']))\n\n    plt.plot([0,1], [0,1], color='orange', linestyle='--')\n\n    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n    plt.xlabel(\"Recall\", fontsize=12)\n\n    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n    plt.ylabel(\"Precision\", fontsize=12)\n\n    plt.title(f'PR Curve Analysis for {gram_name} features', fontweight='bold', fontsize=12)\n    plt.legend(prop={'size':13}, loc='lower right')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.669423Z","iopub.execute_input":"2022-08-10T16:56:47.669770Z","iopub.status.idle":"2022-08-10T16:56:47.739323Z","shell.execute_reply.started":"2022-08-10T16:56:47.669739Z","shell.execute_reply":"2022-08-10T16:56:47.738425Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#data_summary(df)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.740922Z","iopub.execute_input":"2022-08-10T16:56:47.741636Z","iopub.status.idle":"2022-08-10T16:56:47.746365Z","shell.execute_reply.started":"2022-08-10T16:56:47.741563Z","shell.execute_reply":"2022-08-10T16:56:47.745221Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"## classifiers defination\nml_models,model_names = ml_models_for_unigram_tfidf() ","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.750945Z","iopub.execute_input":"2022-08-10T16:56:47.751256Z","iopub.status.idle":"2022-08-10T16:56:47.756307Z","shell.execute_reply.started":"2022-08-10T16:56:47.751227Z","shell.execute_reply":"2022-08-10T16:56:47.755417Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"print(model_names)\nprint(ml_models)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.757647Z","iopub.execute_input":"2022-08-10T16:56:47.758061Z","iopub.status.idle":"2022-08-10T16:56:47.769804Z","shell.execute_reply.started":"2022-08-10T16:56:47.758029Z","shell.execute_reply":"2022-08-10T16:56:47.768693Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# call model accuracy function and save the metrices into a dictionary\naccuracy = {f'{model_names[i]}':model_performace(model,X_train, X_val, y_train, y_val) for i,model in enumerate(ml_models)}\n\n# Save the performance parameter into json file\nwith open('ml_performance_unigram.json', 'w') as f:\n    json.dump(accuracy, f)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:56:47.771223Z","iopub.execute_input":"2022-08-10T16:56:47.771654Z","iopub.status.idle":"2022-08-10T16:57:11.621793Z","shell.execute_reply.started":"2022-08-10T16:56:47.771613Z","shell.execute_reply":"2022-08-10T16:57:11.620543Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Load the json file\naccuracy = json.load(open('ml_performance_unigram.json'))\ntable = performance_table(accuracy)\ntable","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:57:11.623893Z","iopub.execute_input":"2022-08-10T16:57:11.624810Z","iopub.status.idle":"2022-08-10T16:57:11.649738Z","shell.execute_reply.started":"2022-08-10T16:57:11.624754Z","shell.execute_reply":"2022-08-10T16:57:11.648467Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"print(f\"Highest Accuracy achieved by {table.Accuracy.idxmax(axis = 0)} at = {max(table.Accuracy)}\")\nprint(f\"Highest F1-Score achieved by {table['F1 Score'].idxmax(axis = 0)} at = {max(table['F1 Score'] )}\")\nprint(f\"Highest Precision Score achieved by {table['Precision'].idxmax(axis = 0)} at = {max(table['Precision'] )}\")\nprint(f\"Highest Recall Score achieved by {table['Recall'].idxmax(axis = 0)} at = {max(table['Recall'] )}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:57:11.651869Z","iopub.execute_input":"2022-08-10T16:57:11.652794Z","iopub.status.idle":"2022-08-10T16:57:11.663128Z","shell.execute_reply.started":"2022-08-10T16:57:11.652742Z","shell.execute_reply":"2022-08-10T16:57:11.661895Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"\n## classifiers defination\ngram_models = ml_models_for_trigram_tfidf()\n\nplot_roc_curve(gram_models,X_train,X_val,y_train,y_val,'Trigram')","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:57:11.665551Z","iopub.execute_input":"2022-08-10T16:57:11.666512Z","iopub.status.idle":"2022-08-10T16:57:36.257121Z","shell.execute_reply.started":"2022-08-10T16:57:11.666461Z","shell.execute_reply":"2022-08-10T16:57:36.255887Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"#Precision Recall","metadata":{}},{"cell_type":"code","source":"\n## classifiers defination\ngram_models = ml_models_for_trigram_tfidf()\n\nplot_PR_curve(gram_models,X_train,X_val,y_train,y_val,'Trigram')","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:57:36.258383Z","iopub.execute_input":"2022-08-10T16:57:36.258814Z","iopub.status.idle":"2022-08-10T16:57:59.906757Z","shell.execute_reply.started":"2022-08-10T16:57:36.258772Z","shell.execute_reply":"2022-08-10T16:57:59.905595Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n\n# calculate the Unigram Tf-idf feature\ncv,feature_vector = calc_unigram_tfidf(df.headline) \n# Encode the labels\nlables = label_encoding(df.label,False)\n# Split the Feature into train and test set\nX_train,X_test,y_train,y_test = sklearn.model_selection.train_test_split(train_inputs, train_labels, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:59:06.111762Z","iopub.execute_input":"2022-08-10T16:59:06.112288Z","iopub.status.idle":"2022-08-10T16:59:06.262847Z","shell.execute_reply.started":"2022-08-10T16:59:06.112239Z","shell.execute_reply":"2022-08-10T16:59:06.261488Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nmnb_model = MultinomialNB(alpha=0.15)\nmnb_model.fit(X_train,y_train) \ny_pred = mnb_model.predict(X_test)\naccuracy_score(y_true=y_test,y_pred=y_pred)*100","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:59:14.062377Z","iopub.execute_input":"2022-08-10T16:59:14.062791Z","iopub.status.idle":"2022-08-10T16:59:14.075868Z","shell.execute_reply.started":"2022-08-10T16:59:14.062755Z","shell.execute_reply":"2022-08-10T16:59:14.074699Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import pickle\n# open a file, where you ant to store the data\nfile = open('NB.pkl', 'wb')\n\n# dump information to that file\npickle.dump(mnb_model, file)","metadata":{"execution":{"iopub.status.busy":"2022-08-10T16:59:20.079129Z","iopub.execute_input":"2022-08-10T16:59:20.079533Z","iopub.status.idle":"2022-08-10T16:59:20.085988Z","shell.execute_reply.started":"2022-08-10T16:59:20.079498Z","shell.execute_reply":"2022-08-10T16:59:20.084888Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}